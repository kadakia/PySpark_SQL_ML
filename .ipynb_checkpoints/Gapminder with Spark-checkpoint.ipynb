{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work with Gapminder data.  The same dataset is behind our Bokeh app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('gapminder.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----------------+------------------+----------+---------------+------+----------+\n",
      "|    Country|Year|        fertility|              life|population|child_mortality|   gdp|    region|\n",
      "+-----------+----+-----------------+------------------+----------+---------------+------+----------+\n",
      "|Afghanistan|1964|            7.671|            33.639|10474903.0|          339.7|1182.0|South Asia|\n",
      "|Afghanistan|1965|            7.671|            34.152|10697983.0|          334.1|1182.0|South Asia|\n",
      "|Afghanistan|1966|            7.671|            34.662|10927724.0|          328.7|1168.0|South Asia|\n",
      "|Afghanistan|1967|            7.671|             35.17|11163656.0|          323.3|1173.0|South Asia|\n",
      "|Afghanistan|1968|            7.671|            35.674|11411022.0|          318.1|1187.0|South Asia|\n",
      "|Afghanistan|1969|            7.671|            36.172|11676990.0|          313.0|1178.0|South Asia|\n",
      "|Afghanistan|1970|            7.671|36.663000000000004|11964906.0|          307.8|1174.0|South Asia|\n",
      "|Afghanistan|1971|            7.671|            37.143|12273101.0|          302.1|1092.0|South Asia|\n",
      "|Afghanistan|1972|            7.671|37.614000000000004|12593688.0|          296.4|1046.0|South Asia|\n",
      "|Afghanistan|1973|            7.671|            38.075|12915499.0|          290.8|1137.0|South Asia|\n",
      "|Afghanistan|1974|            7.671|            38.529|13223928.0|          284.9|1170.0|South Asia|\n",
      "|Afghanistan|1975|            7.671|            38.977|13505544.0|          279.4|1201.0|South Asia|\n",
      "|Afghanistan|1976|             7.67|            39.417|13766792.0|          273.6|1231.0|South Asia|\n",
      "|Afghanistan|1977|             7.67|            39.855|14003408.0|          267.8|1119.0|South Asia|\n",
      "|Afghanistan|1978|             7.67|            40.298|14179656.0|          261.6|1179.0|South Asia|\n",
      "|Afghanistan|1979|            7.669|            40.756|14249493.0|          255.5|1155.0|South Asia|\n",
      "|Afghanistan|1980|            7.669|            41.242|14185729.0|          249.1|1158.0|South Asia|\n",
      "|Afghanistan|1981|             7.67|             41.77|13984092.0|          242.7|1284.0|South Asia|\n",
      "|Afghanistan|1982|            7.671|            42.347|13672870.0|          236.2|1402.0|South Asia|\n",
      "|Afghanistan|1983|7.672999999999999|            42.977|13300056.0|          229.7|1454.0|South Asia|\n",
      "+-----------+----+-----------------+------------------+----------+---------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+-----------------+------------------+-------------------+-----------------+------------------+------------------+\n",
      "|summary|    Country|              Year|        fertility|              life|         population|  child_mortality|               gdp|            region|\n",
      "+-------+-----------+------------------+-----------------+------------------+-------------------+-----------------+------------------+------------------+\n",
      "|  count|      10111|             10111|            10100|             10111|              10108|             9210|              9000|             10111|\n",
      "|   mean|       null|1988.5145880723965|4.028718761955669| 64.07859968299091|2.560448031559161E7|80.83450488599334|12746.916666666666|              null|\n",
      "| stddev|       null|14.430849463812343|2.013967511764119|11.122778564950746|1.032383024935289E8| 79.2209420722028| 17797.80995341417|              null|\n",
      "|    min|Afghanistan|              1964|            0.836|              16.1|         10001593.0|             10.0|            1000.0|           America|\n",
      "|    max|      Åland|              2013|9.222999999999999|             83.58|         99986136.0|             99.9|           99995.0|Sub-Saharan Africa|\n",
      "+-------+-----------+------------------+-----------------+------------------+-------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('gapminder') # .registerTempTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# *****Converting spark dataframe to pandas dataframe*****\n",
    "import pandas as pd\n",
    "gap_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10111 entries, 0 to 10110\n",
      "Data columns (total 8 columns):\n",
      "Country            10111 non-null object\n",
      "Year               10111 non-null object\n",
      "fertility          10100 non-null object\n",
      "life               10111 non-null object\n",
      "population         10108 non-null object\n",
      "child_mortality    9210 non-null object\n",
      "gdp                9000 non-null object\n",
      "region             10111 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 632.0+ KB\n"
     ]
    }
   ],
   "source": [
    "gap_df.info() # would need to change data types !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10111 entries, 0 to 10110\n",
      "Data columns (total 8 columns):\n",
      "Country            10111 non-null object\n",
      "Year               10111 non-null int64\n",
      "fertility          10100 non-null float64\n",
      "life               10111 non-null float64\n",
      "population         10108 non-null float64\n",
      "child_mortality    9210 non-null float64\n",
      "gdp                9000 non-null float64\n",
      "region             10111 non-null object\n",
      "dtypes: float64(5), int64(1), object(2)\n",
      "memory usage: 632.0+ KB\n"
     ]
    }
   ],
   "source": [
    "gap_df[['Year','fertility','life','population','child_mortality','gdp']] = gap_df[['Year','fertility','life','population','child_mortality','gdp']].apply(pd.to_numeric)\n",
    "gap_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "| Country|\n",
      "+--------+\n",
      "|Paraguay|\n",
      "|Paraguay|\n",
      "|Paraguay|\n",
      "|Paraguay|\n",
      "|Paraguay|\n",
      "|Paraguay|\n",
      "|Paraguay|\n",
      "|Paraguay|\n",
      "|Paraguay|\n",
      "|Paraguay|\n",
      "+--------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# ****Querying a table*****\n",
    "query = \"SELECT Country FROM gapminder WHERE Country like 'Par%' LIMIT 10 \"\n",
    "\n",
    "gapminder10 = spark.sql(query)\n",
    "gapminder10.show()\n",
    "\n",
    "print(type(gapminder10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+------------------+-----------------+----------+---------------+------+-------+\n",
      "| Country|Year|         fertility|             life|population|child_mortality|   gdp| region|\n",
      "+--------+----+------------------+-----------------+----------+---------------+------+-------+\n",
      "|Paraguay|1964|              6.41|64.64399999999999| 2118896.0|           82.0|3163.0|America|\n",
      "|Paraguay|1965|             6.356|           64.779| 2176195.0|           81.3|3256.0|America|\n",
      "|Paraguay|1966|             6.276|           64.904| 2235296.0|           80.6|3203.0|America|\n",
      "|Paraguay|1967|              6.17|           65.031| 2296118.0|           80.1|3315.0|America|\n",
      "|Paraguay|1968|             6.041|            65.17| 2358072.0|           79.4|3339.0|America|\n",
      "|Paraguay|1969|             5.894|65.32300000000001| 2420365.0|           78.7|3375.0|America|\n",
      "|Paraguay|1970|             5.739|           65.487| 2482508.0|           78.0|3487.0|America|\n",
      "|Paraguay|1971|5.5889999999999995|           65.657| 2544362.0|           77.1|3539.0|America|\n",
      "|Paraguay|1972|             5.456|           65.822| 2606357.0|           76.1|3618.0|America|\n",
      "|Paraguay|1973|             5.347|           65.976| 2669285.0|           75.1|3785.0|America|\n",
      "|Paraguay|1974|             5.267|           66.117| 2734247.0|           73.8|3977.0|America|\n",
      "|Paraguay|1975|5.2170000000000005|           66.244| 2802133.0|           72.5|4113.0|America|\n",
      "|Paraguay|1976| 5.196000000000001|           66.361| 2873058.0|           71.1|4284.0|America|\n",
      "|Paraguay|1977|             5.195|           66.471| 2947064.0|           69.6|4632.0|America|\n",
      "|Paraguay|1978| 5.202999999999999|           66.579| 3024857.0|           68.0|5022.0|America|\n",
      "|Paraguay|1979|5.2139999999999995|           66.687| 3107259.0|           66.4|5449.0|America|\n",
      "|Paraguay|1980| 5.218999999999999|           66.794| 3194768.0|           64.7|6087.0|America|\n",
      "|Paraguay|1981| 5.212999999999999|           66.899| 3287677.0|           62.9|6437.0|America|\n",
      "|Paraguay|1982|             5.191|           67.001| 3385626.0|           61.0|6038.0|America|\n",
      "|Paraguay|1983|             5.151|           67.101| 3487626.0|           59.1|5686.0|America|\n",
      "+--------+----+------------------+-----------------+----------+---------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filtering a dataframe (different from querying a table)\n",
    "# can chain filters\n",
    "df.filter(\"Country == 'Paraguay'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----------------+\n",
      "|    Country|Year|        fertility|\n",
      "+-----------+----+-----------------+\n",
      "|Afghanistan|1964|            7.671|\n",
      "|Afghanistan|1965|            7.671|\n",
      "|Afghanistan|1966|            7.671|\n",
      "|Afghanistan|1967|            7.671|\n",
      "|Afghanistan|1968|            7.671|\n",
      "|Afghanistan|1969|            7.671|\n",
      "|Afghanistan|1970|            7.671|\n",
      "|Afghanistan|1971|            7.671|\n",
      "|Afghanistan|1972|            7.671|\n",
      "|Afghanistan|1973|            7.671|\n",
      "|Afghanistan|1974|            7.671|\n",
      "|Afghanistan|1975|            7.671|\n",
      "|Afghanistan|1976|             7.67|\n",
      "|Afghanistan|1977|             7.67|\n",
      "|Afghanistan|1978|             7.67|\n",
      "|Afghanistan|1979|            7.669|\n",
      "|Afghanistan|1980|            7.669|\n",
      "|Afghanistan|1981|             7.67|\n",
      "|Afghanistan|1982|            7.671|\n",
      "|Afghanistan|1983|7.672999999999999|\n",
      "+-----------+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Country\",\"Year\",\"fertility\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---------------+\n",
      "|    Country|Year|      total_gdp|\n",
      "+-----------+----+---------------+\n",
      "|Afghanistan|1964|1.2381335346E10|\n",
      "|Afghanistan|1965|1.2645015906E10|\n",
      "|Afghanistan|1966|1.2763581632E10|\n",
      "|Afghanistan|1967|1.3094968488E10|\n",
      "|Afghanistan|1968|1.3544883114E10|\n",
      "|Afghanistan|1969| 1.375549422E10|\n",
      "|Afghanistan|1970|1.4046799644E10|\n",
      "|Afghanistan|1971|1.3402226292E10|\n",
      "|Afghanistan|1972|1.3172997648E10|\n",
      "|Afghanistan|1973|1.4684922363E10|\n",
      "|Afghanistan|1974| 1.547199576E10|\n",
      "|Afghanistan|1975|1.6220158344E10|\n",
      "|Afghanistan|1976|1.6946920952E10|\n",
      "|Afghanistan|1977|1.5669813552E10|\n",
      "|Afghanistan|1978|1.6717814424E10|\n",
      "|Afghanistan|1979|1.6458164415E10|\n",
      "|Afghanistan|1980|1.6427074182E10|\n",
      "|Afghanistan|1981|1.7955574128E10|\n",
      "|Afghanistan|1982| 1.916936374E10|\n",
      "|Afghanistan|1983|1.9338281424E10|\n",
      "+-----------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define total_gdp\n",
    "total_gdp = (df.population * df.gdp).alias(\"total_gdp\")\n",
    "\n",
    "# Select columns\n",
    "gdp1 = df.select(df.Country, df.Year, total_gdp).show()\n",
    "\n",
    "# Create the same table using SQL \"as\" instead of .alias\n",
    "gdp2 = df.selectExpr(\"Country\", \"Year\", \"(population * gdp) as total_gdp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------------+\n",
      "| Country|Year|      total_gdp|\n",
      "+--------+----+---------------+\n",
      "|Paraguay|1964|  6.702068048E9|\n",
      "|Paraguay|1965|   7.08569092E9|\n",
      "|Paraguay|1966|  7.159653088E9|\n",
      "|Paraguay|1967|   7.61163117E9|\n",
      "|Paraguay|1968|  7.873602408E9|\n",
      "|Paraguay|1969|  8.168731875E9|\n",
      "|Paraguay|1970|  8.656505396E9|\n",
      "|Paraguay|1971|  9.004497118E9|\n",
      "|Paraguay|1972|  9.429799626E9|\n",
      "|Paraguay|1973|1.0103243725E10|\n",
      "|Paraguay|1974|1.0874100319E10|\n",
      "|Paraguay|1975|1.1525173029E10|\n",
      "|Paraguay|1976|1.2308180472E10|\n",
      "|Paraguay|1977|1.3650800448E10|\n",
      "|Paraguay|1978|1.5190831854E10|\n",
      "|Paraguay|1979|1.6931454291E10|\n",
      "|Paraguay|1980|1.9446552816E10|\n",
      "|Paraguay|1981|2.1162776849E10|\n",
      "|Paraguay|1982|2.0442409788E10|\n",
      "|Paraguay|1983|1.9830641436E10|\n",
      "+--------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"like\" is sensitive to capitalization\n",
    "gdp2.filter(\"Country like 'Par%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|max(Year)|\n",
      "+---------+\n",
      "|     2013|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.selectExpr(\"cast(Year as int)\").groupBy().max(\"Year\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IntegerType]\n"
     ]
    }
   ],
   "source": [
    "# df.schema.names\n",
    "types = [f.dataType for f in df2.schema.fields]\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|              region|min(gdp)|\n",
      "+--------------------+--------+\n",
      "|             America|  1518.0|\n",
      "|Middle East & Nor...|  1202.0|\n",
      "|Europe & Central ...|  1040.0|\n",
      "|          South Asia|   725.0|\n",
      "| East Asia & Pacific|   669.0|\n",
      "|  Sub-Saharan Africa|   142.0|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "# Find the lowest gdp, grouped by region and sorted in descending order\n",
    "df3 = df.selectExpr(\"region\",\"cast(gdp as float)\").groupBy(\"region\").min(\"gdp\").sort(desc(\"min(gdp)\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|              region|min_gdp|\n",
      "+--------------------+-------+\n",
      "|             America| 1518.0|\n",
      "|Middle East & Nor...| 1202.0|\n",
      "|Europe & Central ...| 1040.0|\n",
      "|          South Asia|  725.0|\n",
      "| East Asia & Pacific|  669.0|\n",
      "|  Sub-Saharan Africa|  142.0|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# conventional sql query producing same result\n",
    "query = \"select region, min(cast(gdp as float)) as min_gdp from gapminder group by region order by min_gdp desc\"\n",
    "\n",
    "max_gdp = spark.sql(query)\n",
    "max_gdp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------------+\n",
      "|         Country|stddev_samp(child_mortality)|\n",
      "+----------------+----------------------------+\n",
      "|            Chad|          35.739075467982104|\n",
      "|        Paraguay|           20.19840254342217|\n",
      "|          Russia|           9.231132658740492|\n",
      "|Congo, Dem. Rep.|          45.122435491603035|\n",
      "|         Senegal|           77.17819603266939|\n",
      "|    Macao, China|           5.980079296775411|\n",
      "|          Sweden|            4.05458923659981|\n",
      "|         Tokelau|                        null|\n",
      "|        Kiribati|           32.83193697780084|\n",
      "|  Macedonia, FYR|            44.5864241258091|\n",
      "|          Guyana|          13.231326618356647|\n",
      "|         Eritrea|           64.75510396588561|\n",
      "|     Philippines|          21.822733044946958|\n",
      "|        Djibouti|            41.4177919394897|\n",
      "|           Tonga|          15.955612025304482|\n",
      "|        Malaysia|           20.08011759051673|\n",
      "|       Singapore|          10.118960985623545|\n",
      "|            Fiji|          13.395933589514907|\n",
      "|          Turkey|           65.16106227444958|\n",
      "|      Czech Rep.|                        null|\n",
      "+----------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "by_country = df.groupBy(\"Country\")\n",
    "\n",
    "# Standard deviation\n",
    "by_country.agg(F.stddev(\"child_mortality\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification: predicting whether life expectancy is above 75 years, using pyspark.ml pipeline!\n",
    "\n",
    "By logistic regression. Our model will predict the probability of life expectancy being above 75 years.  (Similar to softmax regression I did in keras for digit recognition.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Country', 'string'),\n",
       " ('Year', 'int'),\n",
       " ('fertility', 'float'),\n",
       " ('life', 'float'),\n",
       " ('population', 'float'),\n",
       " ('child_mortality', 'float'),\n",
       " ('gdp', 'float'),\n",
       " ('region', 'string')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"Year\", df.Year.cast(\"integer\"))\n",
    "df = df.withColumn(\"fertility\", df.fertility.cast(\"float\"))\n",
    "df = df.withColumn(\"life\", df.life.cast(\"float\"))\n",
    "df = df.withColumn(\"population\", df.population.cast(\"float\"))\n",
    "df = df.withColumn(\"child_mortality\", df.child_mortality.cast(\"float\"))\n",
    "df = df.withColumn(\"gdp\", df.gdp.cast(\"float\"))\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringType,\n",
       " IntegerType,\n",
       " FloatType,\n",
       " FloatType,\n",
       " FloatType,\n",
       " FloatType,\n",
       " FloatType,\n",
       " StringType]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f.dataType for f in df.schema.fields] # difference between, say, string and StringType ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10111"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() # just checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "reg_indexer = StringIndexer(inputCol = \"region\", outputCol = \"region_index\")\n",
    "\n",
    "reg_encoder = OneHotEncoder(inputCol = \"region_index\", outputCol = \"region_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler(inputCols=[\"Year\", \"fertility\", \"child_mortality\", \"gdp\", \"region_fact\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8836"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create above_75\n",
    "df = df.withColumn(\"above_75\", df.life > 75)\n",
    "\n",
    "# Convert to an integer\n",
    "df = df.withColumn(\"label\", df.above_75.cast(\"integer\"))\n",
    "\n",
    "# Remove missing values\n",
    "df = df.filter(\"fertility is not NULL and gdp is not NULL and child_mortality is not NULL\")\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "df_pipe = Pipeline(stages=[reg_indexer, reg_encoder, vec_assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the data\n",
    "piped_data = df_pipe.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Country',\n",
       " 'Year',\n",
       " 'fertility',\n",
       " 'life',\n",
       " 'population',\n",
       " 'child_mortality',\n",
       " 'gdp',\n",
       " 'region',\n",
       " 'above_75',\n",
       " 'label',\n",
       " 'region_index',\n",
       " 'region_fact',\n",
       " 'features']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piped_data.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5247\n",
      "3589\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "training, test = piped_data.randomSplit([.6, .4])\n",
    "print(training.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression instance\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# lr.explainParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation on training set, and hyperparameter tuning (elasticNetParam and regParam).  Elastic Net is, I believe, a combination of L^1 and L^2 regularizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create a BinaryClassificationEvaluator instance\n",
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "\n",
    "# Another option is area under precision - recall curve\n",
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is created by plotting the true positive rate against the false positive rate at various threshold settings (i.e., various probability cutoffs).  Ideal AUC is 1.  I've seen ROC, along with confusion matrix, in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tuning submodule\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import numpy as np\n",
    "\n",
    "# Create the parameter grid\n",
    "grid = ParamGridBuilder()\n",
    "\n",
    "# Add each hyperparameter\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .02, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0.0,1.0]) # either all L^1, when elasticNetParam = 1, or all L^2, when elasticNetParam = 0\n",
    "\n",
    "# Build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CrossValidator\n",
    "# numFolds = 3, by default\n",
    "cv = CrossValidator(estimator=lr,\n",
    "               estimatorParamMaps=grid,\n",
    "               evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit cross validation models\n",
    "models = cv.fit(training)\n",
    "\n",
    "# Extract the best model\n",
    "best_lr = models.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lr._java_obj.getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lr._java_obj.getElasticNetParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9563231561916211\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the test set\n",
    "test_results = best_lr.transform(test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+--------------------+----------+\n",
      "|             Country|Year|label|         probability|prediction|\n",
      "+--------------------+----+-----+--------------------+----------+\n",
      "|             Austria|2010|    1|[0.27816036575243...|       1.0|\n",
      "|          Bangladesh|2010|    0|[0.92898875397119...|       0.0|\n",
      "|            Barbados|2010|    0|[0.71042576196812...|       0.0|\n",
      "|             Belarus|2010|    0|[0.57575700236623...|       0.0|\n",
      "|              Belize|2010|    0|[0.91366541289979...|       0.0|\n",
      "|               Benin|2010|    0|[0.99893292146753...|       0.0|\n",
      "|             Bolivia|2010|    0|[0.97348423925998...|       0.0|\n",
      "|              Brunei|2010|    1|[0.19527484742281...|       1.0|\n",
      "|            Bulgaria|2010|    0|[0.62042812164220...|       0.0|\n",
      "|            Cameroon|2010|    0|[0.99866104485070...|       0.0|\n",
      "|              Canada|2010|    1|[0.34764474536552...|       1.0|\n",
      "|          Cape Verde|2010|    0|[0.90004004480125...|       0.0|\n",
      "|                Chad|2010|    0|[0.99991359941873...|       0.0|\n",
      "|             Comoros|2010|    0|[0.99798032685147...|       0.0|\n",
      "|          Costa Rica|2010|    1|[0.72288100221047...|       0.0|\n",
      "|             Denmark|2010|    1|[0.38228302935609...|       1.0|\n",
      "|            Djibouti|2010|    0|[0.98910300721053...|       0.0|\n",
      "|   Equatorial Guinea|2010|    0|[0.99561475947562...|       0.0|\n",
      "|                Fiji|2010|    0|[0.91286850382106...|       0.0|\n",
      "|             Finland|2010|    1|[0.41464382274170...|       1.0|\n",
      "|              France|2010|    1|[0.48017399317989...|       1.0|\n",
      "|              Greece|2010|    1|[0.43191039717165...|       1.0|\n",
      "|       Guinea-Bissau|2010|    0|[0.99904413795652...|       0.0|\n",
      "|               Haiti|2010|    0|[0.99866941617353...|       0.0|\n",
      "|            Honduras|2010|    0|[0.95358027180874...|       0.0|\n",
      "|    Hong Kong, China|2010|    1|[0.16548144608778...|       1.0|\n",
      "|               India|2010|    0|[0.95162452775012...|       0.0|\n",
      "|                Iran|2010|    0|[0.72605806778274...|       0.0|\n",
      "|                Iraq|2010|    0|[0.98300772921927...|       0.0|\n",
      "|             Ireland|2010|    1|[0.40802007547194...|       1.0|\n",
      "|               Italy|2010|    1|[0.34326520619676...|       1.0|\n",
      "|              Kuwait|2010|    0|[0.28770697354958...|       1.0|\n",
      "|              Latvia|2010|    0|[0.58875726210316...|       0.0|\n",
      "|             Lesotho|2010|    0|[0.98937615734369...|       0.0|\n",
      "|          Luxembourg|2010|    1|[0.05188626570164...|       1.0|\n",
      "|      Macedonia, FYR|2010|    0|[0.63743404450678...|       0.0|\n",
      "|              Malawi|2010|    0|[0.99920624737520...|       0.0|\n",
      "|            Malaysia|2010|    0|[0.67692981389189...|       0.0|\n",
      "|            Maldives|2010|    1|[0.82990972048328...|       0.0|\n",
      "|                Mali|2010|    0|[0.99990165481269...|       0.0|\n",
      "|          Mauritania|2010|    0|[0.99804787267549...|       0.0|\n",
      "|              Mexico|2010|    1|[0.80527859278692...|       0.0|\n",
      "|Micronesia, Fed. ...|2010|    0|[0.97570005755812...|       0.0|\n",
      "|             Myanmar|2010|    0|[0.91759062461489...|       0.0|\n",
      "|             Namibia|2010|    0|[0.96979594864866...|       0.0|\n",
      "|           Nicaragua|2010|    0|[0.92297755411184...|       0.0|\n",
      "|               Niger|2010|    0|[0.99994784753241...|       0.0|\n",
      "|             Nigeria|2010|    0|[0.99968113571707...|       0.0|\n",
      "|                Oman|2010|    1|[0.62813765651920...|       0.0|\n",
      "|              Panama|2010|    1|[0.85754170882802...|       0.0|\n",
      "+--------------------+----+-----+--------------------+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.select(\"Country\", \"Year\", \"label\", \"probability\", \"prediction\").filter(test_results.Year == 2010).show(50)\n",
    "# \"probability\" is probability that life expectancy <= 75, it seems\n",
    "# threshold probability seems to be 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMS Physician Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phys = spark.read.csv('Physician_Compare_National_Downloadable_File.csv', header = True)\n",
    "# No memory error or warning !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_phys.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NPI',\n",
       " 'PAC ID',\n",
       " 'Professional Enrollment ID',\n",
       " 'Last Name',\n",
       " 'First Name',\n",
       " 'Middle Name',\n",
       " 'Suffix',\n",
       " 'Gender',\n",
       " 'Credential',\n",
       " 'Medical school name',\n",
       " 'Graduation year',\n",
       " 'Primary specialty',\n",
       " 'Secondary specialty 1',\n",
       " 'Secondary specialty 2',\n",
       " 'Secondary specialty 3',\n",
       " 'Secondary specialty 4',\n",
       " 'All secondary specialties',\n",
       " 'Organization legal name',\n",
       " 'Group Practice PAC ID',\n",
       " 'Number of Group Practice members',\n",
       " 'Line 1 Street Address',\n",
       " 'Line 2 Street Address',\n",
       " 'Marker of address line 2 suppression',\n",
       " 'City',\n",
       " 'State',\n",
       " 'Zip Code',\n",
       " 'Phone Number',\n",
       " 'Hospital affiliation CCN 1',\n",
       " 'Hospital affiliation LBN 1',\n",
       " 'Hospital affiliation CCN 2',\n",
       " 'Hospital affiliation LBN 2',\n",
       " 'Hospital affiliation CCN 3',\n",
       " 'Hospital affiliation LBN 3',\n",
       " 'Hospital affiliation CCN 4',\n",
       " 'Hospital affiliation LBN 4',\n",
       " 'Hospital affiliation CCN 5',\n",
       " 'Hospital affiliation LBN 5',\n",
       " 'Professional accepts Medicare Assignment',\n",
       " 'Reported Quality Measures',\n",
       " 'Used electronic health records',\n",
       " 'Committed to heart health through the Million Hearts® initiative.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_phys.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind = spark.read.csv('Physician_Compare_2015_Individual_EP_Public_Reporting___Performance_Scores.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ind.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NPI',\n",
       " 'PAC ID',\n",
       " 'Last Name',\n",
       " 'First Name',\n",
       " 'Measure Identifier',\n",
       " 'Measure Title',\n",
       " 'Inverse Measure',\n",
       " 'Measure Performance Rate',\n",
       " 'Reporting Mechanism',\n",
       " 'Reported on PC Live Site']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ind.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phys = df_phys.withColumnRenamed(\"Last Name\", \"LastName\")\n",
    "df_ind = df_ind.withColumnRenamed(\"Measure Title\", \"MeasureTitle\")\n",
    "df_ind = df_ind.withColumnRenamed(\"Measure Performance Rate\", \"MeasurePerformanceRate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phys.createOrReplaceTempView('physician')\n",
    "df_ind.createOrReplaceTempView('individual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='individual', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='physician', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+------------------------+\n",
      "|       NPI|Last_Name|       Measure_Title|Measure_Performance_Rate|\n",
      "+----------+---------+--------------------+------------------------+\n",
      "|1003817859|   BROWNE|Radiology: Exposu...|                     100|\n",
      "|1003817859|   BROWNE|Radiology: Stenos...|                     100|\n",
      "|1003817859|   BROWNE|Radiology: Exposu...|                     100|\n",
      "|1003817859|   BROWNE|Radiology: Stenos...|                     100|\n",
      "|1003846908|  KAIROUZ|           Care Plan|                      80|\n",
      "|1003846908|  KAIROUZ|Hypertension (HTN...|                      81|\n",
      "|1003846908|  KAIROUZ|           Care Plan|                      80|\n",
      "|1003846908|  KAIROUZ|Hypertension (HTN...|                      81|\n",
      "|1003846908|  KAIROUZ|           Care Plan|                      80|\n",
      "|1003846908|  KAIROUZ|Hypertension (HTN...|                      81|\n",
      "+----------+---------+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# *Querying multiple tables*\n",
    "join_query = \"SELECT physician.NPI, physician.Last_Name, individual.Measure_Title, individual.Measure_Performance_Rate FROM physician JOIN individual ON physician.NPI == individual.NPI LIMIT 10\"\n",
    "\n",
    "cms10 = spark.sql(join_query)\n",
    "cms10.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
